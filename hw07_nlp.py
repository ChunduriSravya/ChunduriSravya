# -*- coding: utf-8 -*-
"""Hw07_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bd9M5Njlm0z-D1-t1mY8WOnTiov6B1uI
"""

import nltk
nltk.download('wordnet')

# Tokenize the documents.
from nltk.tokenize import RegexpTokenizer
from gensim.corpora import Dictionary
from nltk.stem.wordnet import WordNetLemmatizer
from gensim.models import LdaModel

import re
import tarfile

import smart_open


def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):
    with smart_open.open(url, "rb") as file:
        with tarfile.open(fileobj=file) as tar:
            for member in tar.getmembers():
                if member.isfile() and re.search(r'nipstxt/nips\d+/\d+\.txt', member.name):
                    member_bytes = tar.extractfile(member).read()
                    yield member_bytes.decode('utf-8', errors='replace')


docs = list(extract_documents())
# Splitting the documents into tokens.
tokenizer = RegexpTokenizer(r'\w+')
for idx in range(len(docs)):
    docs[idx] = docs[idx].lower()  # Convert to lowercase.
    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.

# Removing numbers, but not words that contain numbers.
docs = [[token for token in doc if not token.isnumeric()] for doc in docs]

# Removing words that are only one character.
docs = [[token for token in doc if len(token) > 1] for doc in docs]

# Lemmatize the documents.
lemmatizer = WordNetLemmatizer()
docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]

#the dictionary for no_above=0.75
dictionary = Dictionary(docs)

# Filtering out words that occur less than 20 documents, or more than 75% of the documents.
dictionary.filter_extremes(no_below=20, no_above=0.75)

# Bag-of-words representation of the documents.
corpus = [dictionary.doc2bow(doc) for doc in docs]
print('Number of unique tokens: %d' % len(dictionary))
print('Number of documents: %d' % len(corpus))
model_75 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=15, passes=20, iterations=400)
topics_75 = model_75.top_topics(corpus)

# Applying the filter with no_above=0.9
dictionary.filter_extremes(no_below=20, no_above=0.9)
corpus = [dictionary.doc2bow(doc) for doc in docs]
model_90 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=15, iterations=400)
top_topics_90 = model_90.top_topics(corpus)

# Applying the filter without no_above
dictionary.filter_extremes(no_below=20)
corpus = [dictionary.doc2bow(doc) for doc in docs]
model_no_above = LdaModel(corpus=corpus, id2word=dictionary, num_topics=15, iterations=400)
top_topics_no_above = model_no_above.top_topics(corpus)

# For num_topics=10
model_10 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, iterations=400)
top_topics_10 = model_10.top_topics(corpus)

# For num_topics=15
model_15 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=15, iterations=400)
top_topics_15 = model_15.top_topics(corpus)

# For num_topics=20
model_20 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, iterations=400)
top_topics_20 = model_20.top_topics(corpus)
def print_top_words(model, num_topics):
    for i in range(num_topics):
        words = model.show_topic(i, topn=10)
        topic_words = ', '.join([word for word, prob in words])
        print(f"Topic {i+1}: {topic_words}")

# For num_topics=10
print("Topics with 10 topics:")
print_top_words(model_10, 10)

# For num_topics=15
print("\nTopics with 15 topics:")
print_top_words(model_15, 15)

# For num_topics=20
print("\nTopics with 20 topics:")
print_top_words(model_20, 20)