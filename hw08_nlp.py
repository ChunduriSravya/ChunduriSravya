# -*- coding: utf-8 -*-
"""HW08_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dEwwYz5WP1Yqjd8U473lUZUkPk9PC4Rv
"""

!pip install huggingface_hub
!pip install transformers
!pip install datasets

# datasets
import datasets
import transformers
import torch

datasets.list_datasets()

emotions = datasets.load_dataset('emotion')

emotions

print(emotions['train']['text'][3])
print(emotions['train']['label'][3])

emotions.set_format(type='pandas')
df = emotions['train'][:]
df.head()

df.groupby('label').count().plot.barh()

# Tokenization
model_pkg = 'distilbert-base-uncased'
distilbert_tokenizer = transformers.AutoTokenizer.from_pretrained(model_pkg)

def tokenize(x):
    return distilbert_tokenizer(x['text'], padding=True, truncation=True)
emotions.set_format(type=None)
print(tokenize(emotions['train'][:2]))
print(emotions['train'][:2])

encoded_emotions = emotions.map(tokenize, batched=True)

encoded_emotions

# using a pretrained model
# If you have a gpu use it!
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = transformers.AutoModel.from_pretrained(model_pkg).to(device)

text = 'this is a test'
inputs = distilbert_tokenizer(text, return_tensors='pt')
inputs

with torch.no_grad():
    outputs = model(**inputs)
outputs

def hidden_states(x):
    inputs = {k:v.to(device) for k,v in x.items()
              if k in distilbert_tokenizer.model_input_names}
    with torch.no_grad():
        hidden_state = model(**inputs).last_hidden_state
    return {'hidden_state': hidden_state[:,0].cpu().numpy()}

encoded_emotions

encoded_emotions.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
hidden_emotions = encoded_emotions.map(hidden_states, batched=True)

hidden_emotions['train']['hidden_state']

X_train = hidden_emotions['train']['hidden_state'].numpy()
y_train = hidden_emotions['train']['label'].numpy()
X_validation = hidden_emotions['validation']['hidden_state'].numpy()
y_validation = hidden_emotions['validation']['label'].numpy()

!pip install umap-learn

from sklearn.manifold import TSNE
from sklearn.preprocessing import MinMaxScaler
import umap
import pandas as pd
umapper = umap.UMAP(n_components=2)
tsne = TSNE(n_components=2)
X_scaled = MinMaxScaler().fit_transform(X_train)
X_tsne = tsne.fit_transform(X_scaled)
X_umap = umapper.fit_transform(X_scaled)
df = pd.DataFrame()
df['x']=X_tsne[:,0]
df['y']=X_tsne[:,1]
df['label']=y_train

from matplotlib import pyplot as plt
fig, axes = plt.subplots(2, 3 , figsize=(7,5))
axes = axes.flatten()
cmaps = ['Greys', 'Blues', 'Oranges', 'Reds', 'Greens']
labels = emotions['train'].features['label'].names
for i, (label, cmap) in enumerate(zip(labels, cmaps)):
    df_sub = df[df['label']==labels.index(label)]
    axes[i].set_xlim((df.x.min(), df.x.max()))
    axes[i].set_ylim((df.y.min(), df.y.max()))
    axes[i].hexbin(df_sub['x'], df_sub['y'], cmap=cmap)
    axes[i].set_title(label)
plt.show()

df = pd.DataFrame()
df['x']=X_umap[:,0]
df['y']=X_umap[:,1]
df['label']=y_train

from matplotlib import pyplot as plt
fig, axes = plt.subplots(2, 3 , figsize=(7,5))
axes = axes.flatten()
cmaps = ['Greys', 'Blues', 'Oranges', 'Reds', 'Greens']
labels = emotions['train'].features['label'].names
for i, (label, cmap) in enumerate(zip(labels, cmaps)):
    df_sub = df[df['label']==labels.index(label)]
    axes[i].set_xlim((df.x.min(), df.x.max()))
    axes[i].set_ylim((df.y.min(), df.y.max()))
    axes[i].hexbin(df_sub['x'], df_sub['y'], cmap=cmap)
    axes[i].set_title(label)
plt.show()

from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score

# Fit MinMaxScaler to training data
scaler = MinMaxScaler()
scaler.fit(X_train)

# Transform training and validation data
X_train_scaled = scaler.transform(X_train)
X_validation_scaled = scaler.transform(X_validation)

# Train Logistic Regression classifier
lr = LogisticRegression(max_iter=500)
lr.fit(X_train_scaled, y_train)

# Evaluate Logistic Regression
accuracy = accuracy_score(lr.predict(X_validation_scaled), y_validation)
f1 = f1_score(lr.predict(X_validation_scaled), y_validation, average='weighted')

print("Accuracy:", accuracy)
print("F1 Score:", f1)

from sklearn.ensemble import RandomForestClassifier

# Train Random Forest classifier
rf = RandomForestClassifier()
rf.fit(X_scaled, y_train)

# Evaluate Random Forest
rf_accuracy = accuracy_score(rf.predict(X_validation_scaled), y_validation)
rf_f1_score = f1_score(rf.predict(X_validation_scaled), y_validation, average='weighted')

print("Random Forest Accuracy:", rf_accuracy)
print("Random Forest F1 Score:", rf_f1_score)

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

# Define hyperparameters to search
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Perform randomized search
rf_random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(),
    param_distributions=param_dist,
    n_iter=10,  # Limit the number of iterations for faster execution
    cv=5,
    random_state=42,
    n_jobs=-1
)
rf_random_search.fit(X_scaled, y_train)

# Get best parameters
best_params = rf_random_search.best_params_
print("Best Parameters:", best_params)

# Evaluate fine-tuned Random Forest
rf_fine_tuned_accuracy = accuracy_score(rf_random_search.predict(X_validation_scaled), y_validation)
rf_fine_tuned_f1_score = f1_score(rf_random_search.predict(X_validation_scaled), y_validation, average='weighted')

print("Fine-tuned Random Forest Accuracy:", rf_fine_tuned_accuracy)
print("Fine-tuned Random Forest F1 Score:", rf_fine_tuned_f1_score)

from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("crcb/autotrain-isear_bert-786224257")
tokenizer = AutoTokenizer.from_pretrained("crcb/autotrain-isear_bert-786224257")

from transformers import AutoTokenizer

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("crcb/autotrain-isear_bert-786224257")

# Tokenize input text
inputs = tokenizer("I love AutoTrain", return_tensors="pt")

# Forward pass through the model
outputs = model(**inputs)
outputs

from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Load the pretrained model
model = AutoModelForSequenceClassification.from_pretrained("https://huggingface.co/local_directory_path/resolve/main/config.json
")

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("https://huggingface.co/local_directory_path/resolve/main/config.json
")